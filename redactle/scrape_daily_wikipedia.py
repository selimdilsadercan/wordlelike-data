"""
days.json dosyasÄ±ndaki gÃ¼nleri sÄ±rayla iÅŸleyip Wikipedia sayfalarÄ±nÄ± scrape eder.
Mevcut scrape_wikipedia.py modÃ¼lÃ¼nÃ¼ kullanÄ±r.
"""

import json
import os
import time
import sys
from scrape_wikipedia import scrape_wikipedia, extract_morphology_metadata
from zeyrek import MorphAnalyzer
from urllib.parse import unquote
import re


def load_days(json_path: str) -> list:
    """days.json dosyasÄ±nÄ± yÃ¼kle"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def create_safe_filename(title: str) -> str:
    """TÃ¼rkÃ§e karakterleri ASCII'ye Ã§evirip gÃ¼venli dosya adÄ± oluÅŸtur"""
    safe_title = title.replace('Ä°', 'I').replace('Ä±', 'i').replace('Å', 'S').replace('ÅŸ', 's')
    safe_title = safe_title.replace('Ä', 'G').replace('ÄŸ', 'g').replace('Ãœ', 'U').replace('Ã¼', 'u')
    safe_title = safe_title.replace('Ã–', 'O').replace('Ã¶', 'o').replace('Ã‡', 'C').replace('Ã§', 'c')
    safe_title = re.sub(r'[^\w\s-]', '', safe_title).strip()
    safe_title = re.sub(r'[-\s]+', '_', safe_title)
    return safe_title


def scrape_all_days(days: list, output_dir: str, delay: float = 1.5, start_from: int = 1):
    """
    TÃ¼m gÃ¼nleri sÄ±rayla scrape et
    
    Args:
        days: days.json'dan yÃ¼klenen liste
        output_dir: Markdown dosyalarÄ±nÄ±n kaydedileceÄŸi klasÃ¶r
        delay: Her istek arasÄ±nda beklenecek sÃ¼re (saniye)
    """
    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    os.makedirs(output_dir, exist_ok=True)
    
    # Morfoloji analizÃ¶rÃ¼nÃ¼ baÅŸlat
    print("Morfoloji analizÃ¶rÃ¼ baÅŸlatÄ±lÄ±yor...")
    analyzer = MorphAnalyzer()
    
    total = len(days)
    success_count = 0
    error_count = 0
    
    print(f"\nToplam {total} gÃ¼n, {start_from}'den baÅŸlanacak...")
    print("=" * 60)
    
    for i, day_data in enumerate(days, 1):
        # Belirtilen indexten Ã¶ncekileri atla
        if i < start_from:
            continue
        day = day_data.get('day', '')
        word = day_data.get('word', '')
        wiki_url = day_data.get('wiki_url', '')
        
        print(f"\n[{i}/{total}] {day} - {word}")
        print(f"  URL: {wiki_url}")
        
        if not wiki_url:
            print("  âš ï¸ URL bulunamadÄ±, atlanÄ±yor...")
            error_count += 1
            continue
        
        try:
            # Wikipedia sayfasÄ±nÄ± scrape et
            content = scrape_wikipedia(wiki_url)
            
            if content:
                # Morfoloji metadata'sÄ±nÄ± Ã§Ä±kar
                lemmas_metadata = extract_morphology_metadata(content, analyzer)
                metadata_json = json.dumps(lemmas_metadata, ensure_ascii=False, indent=2)
                
                # Dosya adÄ± oluÅŸtur: 001-26.11.2025.md formatÄ±nda
                md_filename = f"{i:03d}-{day}.md"
                filepath = os.path.join(output_dir, md_filename)
                
                # Markdown dosyasÄ±na kaydet
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                    f.write('\n\n---\n\n')
                    f.write('```json\n')
                    f.write('lemmas = ')
                    f.write(metadata_json)
                    f.write('\n```\n')
                
                print(f"  âœ… Kaydedildi: {md_filename}")
                print(f"     Karakter: {len(content)}, Kelime: {len(content.split())}, KÃ¶k: {len(lemmas_metadata)}")
                success_count += 1
            else:
                print("  âŒ Ä°Ã§erik alÄ±namadÄ±!")
                error_count += 1
                
        except Exception as e:
            print(f"  âŒ Hata: {str(e)}")
            error_count += 1
        
        # Rate limiting - Wikipedia'yÄ± yormamak iÃ§in
        if i < total:
            print(f"  â³ {delay} saniye bekleniyor...")
            time.sleep(delay)
    
    print("\n" + "=" * 60)
    print(f"TAMAMLANDI!")
    print(f"  âœ… BaÅŸarÄ±lÄ±: {success_count}")
    print(f"  âŒ Hata: {error_count}")
    print(f"  ğŸ“ Ã‡Ä±ktÄ± klasÃ¶rÃ¼: {output_dir}")


def main():
    # Mevcut dizin
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # days.json yolu
    days_json_path = os.path.join(script_dir, 'days.json')
    
    # Ã‡Ä±ktÄ± dizini
    output_dir = os.path.join(script_dir, 'data')
    
    # GÃ¼nleri yÃ¼kle
    print("days.json yÃ¼kleniyor...")
    days = load_days(days_json_path)
    print(f"âœ… {len(days)} gÃ¼n yÃ¼klendi.")
    
    # TÃ¼m gÃ¼nleri scrape et
    # 17'den devam et (ilk 16 zaten yapÄ±ldÄ±)
    scrape_all_days(days, output_dir=output_dir, delay=1.5, start_from=17)


if __name__ == '__main__':
    main()
